from typing import List, Optional, Sequence
from langchain.docstore.document import Document
import os
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import CrossEncoder
from langchain_core.documents.compressor import BaseDocumentCompressor
from langchain_core.callbacks import Callbacks
from pydantic import Field, PrivateAttr
from langchain.schema import Document
from langchain_community.retrievers import BM25Retriever
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import re
from smart_fridge_tracker import get_display_names_from_file


class FastKeywordRetriever:
    def __init__(self, docs, top_k=10):
        self.docs = docs
        self.top_k = top_k

    def invoke(self, query: str):
        pattern = re.compile(re.escape(query))  # ç¼–è¯‘æ­£åˆ™åŒ¹é…
        hits = [doc for doc in self.docs if pattern.search(doc.page_content)]
        return hits[:self.top_k]

class CrossEncoderReranker(BaseDocumentCompressor):
    #å°†top_kå£°æ˜ä¸ºPydantic å­—æ®µ
    top_k: int = Field(1, description="Number of top documents to return")
    #å£°æ˜ä¸€ä¸ªç§æœ‰å±æ€§æ¥å­˜å‚¨CrossEncoder
    _reranker: CrossEncoder = PrivateAttr()

    def __init__(
        self,
        model_name_or_path: str = "cross-encoder/ms-marco-MiniLM-L-12-v2",
        top_k: int = 3,
        use_gpu: bool = True,
        **kwargs
    ):
        """Pydantic è¦æ±‚åœ¨ super().__init__ ä¸­æ˜¾å¼ä¼ å…¥å·²å£°æ˜å­—æ®µ (å¦‚ top_k)ã€‚"""
        super().__init__(top_k=top_k, **kwargs)

        device = "cuda" if use_gpu else "cpu"
        #å°† CrossEncoderæ¨¡å‹å­˜å‚¨åˆ°ç§æœ‰å±æ€§é‡Œ
        self._reranker = CrossEncoder(model_name_or_path, device=device)

    def rerank(self, query: str, docs: List[Document]) -> List[Document]:
        if not docs:
            return []
        pairs = [(query, d.page_content) for d in docs]
        scores = self._reranker.predict(pairs)
        docs_with_scores = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
        top_docs = [d for d, s in docs_with_scores[: self.top_k]]
        return top_docs

    def compress_documents(
        self, documents: Sequence[Document], query: str, callbacks: Optional[Callbacks] = None
    ) -> Sequence[Document]:
        """Required by BaseDocumentCompressor; delegates to `rerank`."""
        return self.rerank(query, list(documents))
    
def build_langchain_pipeline(
    documents: List[Document],
    default_prompt_template: str,
    top_k: int = 5,
):
    #fewshotpipeline = FewShotPipeline(q_list, a_list, topk)
    # bm25_retriever = BM25Retriever.from_documents(documents,k=100)#input: List[Document],run :query:str->List[Document]
    # reranker1 = CrossEncoderReranker(top_k = top_k)

    # reranker = ContextualCompressionRetriever(
    #     base_compressor=reranker1, base_retriever=bm25_retriever
    # )
    reranker=FastKeywordRetriever(documents)
    llm = ChatOpenAI(
        model_name="gpt-4o",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0,
        max_tokens=4096
    )

    prompt_temp = PromptTemplate(
        template=default_prompt_template,
        input_variables=["context", "question"],
    )
    chain = prompt_temp | llm | StrOutputParser()

    def run(query: str) -> str:
        reranked_docs = reranker.invoke(query)
        reranked_docs = [d.page_content for d in reranked_docs]
        
        top_docs = reranked_docs[:top_k]
        ##############################æ¶ˆèå®éªŒ
        # top_docs = reranked_docs[:0]
        print('here is top documents', top_docs,'++++++')
        context = "\n\n".join(doc for doc in top_docs)
        #few_shot_example = fewshotpipeline.run(query)#è¿›å…¥queryè¾“å‡ºç›¸å…³questionçš„qaå¯¹
        #return chain.invoke({"context": context, "question": query, "few_shot_example": few_shot_example})
        return chain.invoke({"context": context, "question": query})

    return run
def load_txt_files(folder_path):
    docs = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".txt"):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read().strip()
                        if content:
                            docs.append(Document(page_content=content, metadata={"source": file_path}))
                except Exception as e:
                    print(f"fail to load {file_path} error: {e}")

    return docs

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

class ParagraphThenCharacterSplitter:
    def __init__(self):
        pass  # å¦‚æœä¸éœ€è¦å†å¤„ç† chunk_size å’Œ chunk_overlapï¼Œå¯å»æ‰æˆ–ç•™ç©º

    def split_document(self, doc):
        # ä»…æŒ‰ç…§ä¸¤ä¸ªæ¢è¡Œç¬¦åˆ†å‰²æ–‡æ¡£
        paragraphs = doc.page_content.split("\n\n")
        final_chunks = []

        for paragraph in paragraphs:
            # å»é™¤å‰åç©ºç™½
            paragraph = paragraph.strip()
            # å¦‚æœå½“å‰æ®µè½éç©ºï¼Œåˆ™ä½œä¸ºä¸€ä¸ª chunk
            if paragraph:
                final_chunks.append(
                    Document(page_content=paragraph, metadata=doc.metadata)
                )

        return final_chunks

    def process(self, docs):
        processed_docs = []
        for doc in docs:
            processed_docs.extend(self.split_document(doc))
        return processed_docs

def save_list_to_txt(sentences, file_path, encoding="utf-8"):
    try:
        with open(file_path, "w", encoding=encoding) as f:
            for sentence in sentences:
                f.write(sentence.strip() + "\n")
        print(f"successfully save the txt: {file_path}")
    except Exception as e:
        print(f"fail to save, error: {e}")

import json
def extract_answer(text):
    text = text.split("Answer:", 1)[-1].strip()
    #return text.split("\n\n", 1)[0].strip()
    return text
if __name__ == "__main__":
    folder_path = "RAG\scrape"
    docs = load_txt_files(folder_path)
    docs_processed = ParagraphThenCharacterSplitter().process(docs)
    sample_docs = docs_processed
    default_prompt = ('''åœ¨ä¸‹é¢çš„æ–‡æœ¬ä¸­ï¼Œæå–å‡ºæ‰€æœ‰çš„èœè°±ä¿¡æ¯ã€‚æ¯ä¸ªèœè°±çš„ä¿¡æ¯åŒ…æ‹¬ï¼šèœè°±ç¼–å·ã€èœåã€é£Ÿæã€‚
                      èœè°±ä¸åŒ…å«å¡è·¯é‡Œï¼Œç¢³è¶³è¿¹ã€è›‹ç™½è´¨ã€è„‚è‚ªã€ç¢³æ°´åŒ–åˆç‰©ã€çº¤ç»´ç´ ç­‰ä¿¡æ¯ï¼Œè¯·ä½ æ ¹æ®é£Ÿæè‡ªè¡Œè®¡ç®—ç»™å‡ºå¹¶æ”¾è¿›è¾“å‡ºçš„jsonæ–‡ä»¶è€Œä¸æ˜¯nullã€‚
                      è¯·å°†æå–å‡ºçš„ä¿¡æ¯ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ¯ä¸ªèœè°±çš„ä¿¡æ¯ç”¨å¤§æ‹¬å·æ‹¬èµ·æ¥ï¼Œå¤šä¸ªèœè°±ä¹‹é—´ç”¨é€—å·åˆ†éš”ã€‚
    Example:
    Question: ç‰›è‚‰
    Context: èœè°±ç¼–å·ï¼š663290ï¼Œèœåï¼šèƒ¡èåœç‰›è‚‰èŠ±å·ã€‚é£ŸæåŒ…æ‹¬ï¼šä¸»æ–™ï¼šèƒ¡èåœï¼ˆé€‚é‡ï¼‰ã€ä¸»æ–™ï¼šç‰›è‚‰ï¼ˆ120gï¼‰ã€é¢ç²‰é…æ¯”â¤ï¸ï¼šä¸­ç­‹é¢ç²‰ï¼ˆ300gï¼‰ã€é¢ç²‰é…æ¯”â¤ï¸ï¼šç™½ç³–ï¼ˆ3gï¼‰ã€é¢ç²‰é…æ¯”â¤ï¸ï¼šé…µæ¯ï¼ˆ3gï¼‰ã€é¢ç²‰é…æ¯”â¤ï¸ï¼šæ¸…æ°´ï¼ˆ170mlï¼‰ã€ç‰›è‚‰é…æ–™ï¼šç›ï¼ˆ1gï¼‰ã€ç‰›è‚‰é…æ–™ï¼šç™½èƒ¡æ¤’ç²‰ï¼ˆ1gï¼‰ã€ç‰›è‚‰é…æ–™ï¼šç”ŸæŠ½ï¼ˆ3mlï¼‰ã€ç‰›è‚‰é…æ–™ï¼šæ·€ç²‰ï¼ˆ1gï¼‰ã€‚åšæ³•æ­¥éª¤å¦‚ä¸‹ï¼šå‡†å¤‡ä¸»è¦é£Ÿæï¼šç‰›é‡Œè„Šè‚‰é¦…120å…‹ï¼Œèƒ¡èåœåˆ‡ç¢å°‘è®¸ï¼Œä¸­ç­‹é¢ç²‰300å…‹ã€è‚‰é¦…ä¸­åŠ ç›ï¼Œç‰ç±³æ·€ç²‰ï¼Œç™½èƒ¡æ¤’ç²‰å…ˆè…Œåˆ¶ä¸€ä¸‹ï¼Œæœ€åæ”¾é£Ÿç”¨æ²¹æ··åˆã€é”…ä¸­æ”¾æ²¹ï¼Œè‚‰é¦…ç‚’ä¸€ä¸‹ç››å‡ºæ¥å¤‡ç”¨ã€å…‹é¢ç²‰+3å…‹é…µæ¯+3å…‹ç™½ç³–+170æ¯«å‡æ¸…æ°´ï¼Œæ··åˆæ‰æˆå…‰æ»‘çš„é¢å›¢ã€æ“€æˆé•¿æ–¹å½¢ï¼Œæ”¾è‚‰æ²«å’Œèƒ¡èåœç¢ï¼Œå¯å†æ’’å°‘é‡ç›ã€ä¸Šä¸‹å¯¹æŠ˜èµ·æ¥ã€ä¸¤ä¸ªå ä¸€èµ·ã€ä»ä¸­é—´å‹ä¸€ä¸‹ã€æŒ‰ç…§è‡ªå·±çš„æ‰‹æ³•ä¸¤å¤´å·èµ·æ¥ï¼Œæ”¾æ¸©æš–å¤„å‘é…µã€å‘é…µå¥½çš„èŠ±å·æ˜æ˜¾å˜å¤§å˜è½»ï¼Œæ˜¯ä¹‹å‰çš„1.5å€å¤§ï¼Œè½»è½»æŒ‰å‹è¡¨é¢ä¼šå›å¼¹ï¼Œæ°´å¼€åè’¸é”…ä¸­è’¸15åˆ†é’Ÿã€æ¾è½¯å¤§èŠ±å·å‡ºé”…äº†ï¼Œç‚’ç†Ÿçš„ç‰›è‚‰ç¢å·èŠ±å·çœŸçš„å¾ˆé¦™â€¦å¯ä»¥è¯•è¯•ï¼Œå­©å­å–œæ¬¢ï¼Œæˆ‘å¿ä¸ä½åƒäº†ä¸¤ä¸ªğŸ˜Šã€‚
            èœè°±ç¼–å·ï¼š663864ï¼Œèœåï¼šå†·åƒç‰›è‚‰ç‹¬å®¶ã€‚é£ŸæåŒ…æ‹¬ï¼šä¸»æ–™ï¼šç‰›å‰è…¿è‚‰ï¼ˆä¸€æ–¤ï¼‰ã€è¾…æ–™ï¼šè‘±å§œè’œï¼ˆé€‚é‡ï¼‰ã€è¾…æ–™ï¼šæ´‹è‘±ï¼ˆä¸€ä¸ªï¼‰ã€è¾…æ–™ï¼šå°è‘±ï¼ˆä¸€æŠŠï¼‰ã€è¾…æ–™ï¼šé¦™èœï¼ˆä¸€æŠŠï¼‰ã€è¾…æ–™ï¼šå¹²è¾£æ¤’ï¼ˆåŠç¢—ï¼‰ã€è¾…æ–™ï¼šçº¢èŠ±æ¤’ï¼ˆä¸€åŒ™ï¼‰ã€è¾…æ–™ï¼šé’èŠ±æ¤’ï¼ˆä¸€åŒ™ï¼‰ã€è¾…æ–™ï¼šå…«è§’ï¼ˆ3ä¸ªï¼‰ã€è¾…æ–™ï¼šé¦™å¶ï¼ˆ3ç‰‡ï¼‰ã€è°ƒæ–™ï¼šèœç±½æ²¹ï¼ˆ2å‹ºï¼‰ã€è°ƒæ–™ï¼šèŠ±é›•é…’ï¼ˆ1åŒ™ï¼‰ã€è°ƒæ–™ï¼šè¾£æ¤’é¢ï¼ˆ2åŒ™ï¼‰ã€è°ƒæ–™ï¼šç”ŸæŠ½ï¼ˆ2åŒ™ï¼‰ã€è°ƒæ–™ï¼šç›ï¼ˆå°è®¸ï¼‰ã€è°ƒæ–™ï¼šå¤æ–™åŒ…ï¼ˆä¸€åŒ…ï¼‰ã€è°ƒæ–™ï¼šç™½ç³–ï¼ˆåŠåŒ™ï¼‰ã€è°ƒæ–™ï¼šå­œç„¶ç²‰ï¼ˆ1åŒ™ï¼‰ã€‚åšæ³•æ­¥éª¤å¦‚ä¸‹ï¼šç‰›å‰è…¿è‚‰æ³¡å‡ºè¡€æ°´20åˆ†é’Ÿã€‚æ¸…æ´—å¹²å‡€ã€å†·æ°´ä¸‹é”…ã€‚åŠ å…¥èŠ±é›•é…’å»è…¥ã€‚æ°´å¼€æ‰“å…¥æµ®æ²«ã€åŠ å°‘è®¸çš„ç›å…¥ä¸ªåº•å‘³ã€‚åŠ å…¥ä¸€ä¸ªé…ç½®å¥½çš„å¤æ–™åŒ…ã€‚å…«è§’é¦™å¶è‘±å§œä¸­å¤§ç«æŠŠè¿™ä¸ªç‰›è‚‰ç…®è‡³å®Œå…¨ç†Ÿé€ã€‚æ˜¯ä¹Ÿä¸èƒ½ç…®çš„å¤ªè½¯äº†ï¼Œç¨å¾®ç¡¬ä¸€äº›æœ‰åš¼åŠ²ã€ç…®ç‰›è‚‰çš„æ—¶å€™æˆ‘ä»¬æ¥ç‚¸ä¸€ä¸ªåº•æ²¹ã€‚èœç±½æ²¹äº”æˆæ²¹æ¸©å€’å…¥è‘±å§œè’œã€‚é¦™èœï¼Œæ´‹è‘±ï¼Œä¸­å°ç«æŠŠè¿™è°ƒæ–™ç‚’é¦™ã€å¹²è¾£æ¤’å‰ªæ–­ï¼ŒåŠ é’çº¢èŠ±æ¤’å¢é¦™ï¼Œæ¸©æ°´æµ¸æ³¡ï¼Œä»¥å…ç‚¸ç³Šã€é”…é‡Œçš„è¿™äº›è°ƒæ–™å…¨éƒ¨ç‚¸æˆå¹²é…¥ç‚¸å¹²å°±æŠŠè°ƒæ–™æå‡ºï¼Œç•™åº•æ²¹ã€ç…®å¥½çš„ç‰›è‚‰æ”¾ç½®å®Œå…¨å†·å´åé¡ºç€è°ƒç†åˆ‡æˆæ¡ä¸€å®šè¦å†·å´ä»¥åæ‰å¥½åˆ‡ã€ç”¨æ‰‹æŠŠå®ƒæ’•æˆä¸€æ¡ä¸€æ¡çš„ã€‚æˆ‘åæ­£å–œæ¬¢ç”¨æ‰‹æ’•ã€‚è¿™æ ·æ›´åŠ å…¥å‘³ã€‚ä¸å–œæ¬¢çš„ç›´æ¥ç”¨åˆ€åˆ‡æˆåˆ‡æˆæ¡æ¡ã€ç‚¸å¥½çš„æ²¹ï¼Œäº”æˆæ²¹æ¸©æ”¾å…¥ç‰›è‚‰ä¸­å°ç«ã€‚æŠŠç‰›è‚‰ç…¸è‡³ç„¦é¦™ï¼Œç„¦é¦™ã€æ”¾å…¥è¾£æ¤’é¢ã€‚ç¿»ç‚’å‡åŒ€ã€å€’å…¥æ³¡å¥½çš„è¾£æ¤’ï¼Œè’œå’ŒèŠ±æ¤’å€’å…¥ç”ŸæŠ½ï¼Œç™½ç³–æå‘³ã€æœ€åèµ·é”…å‰æ”¾å…¥ç™½èŠéº»ï¼Œå­œç„¶ç²‰ã€‚å°±å¯ä»¥å•¦ã€‚èƒ½å¤Ÿåœ¨é”…é‡Œæµ¸æ³¡ä¸€ä¼šå„¿æ›´å¥½åƒã€‚å½“ä¸»é£Ÿæˆ–è€…å½“é›¶é£Ÿåƒéƒ½ä¸é”™ï¼Œå¯ä»¥ä¸€æ¬¡å¤šç‚’ç‚¹å„¿ï¼Œæ”¾çœŸç©ºåŒ…è£…ã€‚
    Answer: [{{
        "èœè°±ç¼–å·": 663290,
        "èœå": "èƒ¡èåœç‰›è‚‰èŠ±å·",
        "å¡è·¯é‡Œ": "1000å¤§å¡",
        "ç¢³è¶³è¿¹": "0.5 åƒå…‹äºŒæ°§åŒ–ç¢³å½“é‡"
        "é£Ÿæ": [
            "èƒ¡èåœï¼ˆé€‚é‡ï¼‰",
            "ç‰›è‚‰ï¼ˆ120gï¼‰",
            "ä¸­ç­‹é¢ç²‰ï¼ˆ300gï¼‰",
            "ç™½ç³–ï¼ˆ3gï¼‰",
            "é…µæ¯ï¼ˆ3gï¼‰",
            "æ¸…æ°´ï¼ˆ170mlï¼‰",
            "ç›ï¼ˆ1gï¼‰",
            "ç™½èƒ¡æ¤’ç²‰ï¼ˆ1gï¼‰",
            "ç”ŸæŠ½ï¼ˆ3mlï¼‰",
            "æ·€ç²‰ï¼ˆ1gï¼‰"],
        "è›‹ç™½è´¨": "20g",
        "è„‚è‚ª": "10g",
        "ç¢³æ°´åŒ–åˆç‰©": "150g",
        "çº¤ç»´ç´ ": "5g",
        ,
    }},
    ...(çœç•¥å…¶ä»–èœè°±)...
    {{
        "èœè°±ç¼–å·": 663864,
        "èœå": "å†·åƒç‰›è‚‰ç‹¬å®¶",
        "å¡è·¯é‡Œ": "800å¤§å¡",
        "ç¢³è¶³è¿¹": "0.8 åƒå…‹äºŒæ°§åŒ–ç¢³å½“é‡"
        "é£Ÿæ": [
            "ç‰›å‰è…¿è‚‰ï¼ˆä¸€æ–¤ï¼‰",
            "è‘±å§œè’œï¼ˆé€‚é‡ï¼‰",
            "æ´‹è‘±ï¼ˆä¸€ä¸ªï¼‰",
            "å°è‘±ï¼ˆä¸€æŠŠï¼‰",
            "é¦™èœï¼ˆä¸€æŠŠï¼‰",
            "å¹²è¾£æ¤’ï¼ˆåŠç¢—ï¼‰",
            "çº¢èŠ±æ¤’ï¼ˆä¸€åŒ™ï¼‰",
            "é’èŠ±æ¤’ï¼ˆä¸€åŒ™ï¼‰",
            "å…«è§’ï¼ˆ3ä¸ªï¼‰",
            "é¦™å¶ï¼ˆ3ç‰‡ï¼‰",
            "èœç±½æ²¹ï¼ˆ2å‹ºï¼‰",
            "èŠ±é›•é…’ï¼ˆ1åŒ™ï¼‰",
            "è¾£æ¤’é¢ï¼ˆ2åŒ™ï¼‰",
            "ç”ŸæŠ½ï¼ˆ2åŒ™ï¼‰",
            "ç›ï¼ˆå°è®¸ï¼‰",
            "å¤æ–™åŒ…ï¼ˆä¸€åŒ…ï¼‰",
            "ç™½ç³–ï¼ˆåŠåŒ™ï¼‰",
            "å­œç„¶ç²‰ï¼ˆ1åŒ™ï¼‰"],
        "è›‹ç™½è´¨": "40g",
        "è„‚è‚ª": "30g",
        "ç¢³æ°´åŒ–åˆç‰©": "20g",
        "çº¤ç»´ç´ ": "3g",
    }}]

    Question: {question}
    Context: {context}
    Answer:'''
    )
    pipeline_fn = build_langchain_pipeline(
        documents=sample_docs,
        default_prompt_template=default_prompt
    )
    # answer = pipeline_fn("ä¸»æ–™ï¼šçŒªè‚‰")
    # answer = extract_answer(answer)
    # print('____________',answer)
    # è¾“å…¥å…³é”®è¯åˆ—è¡¨
    keywords = get_display_names_from_file("fridge_inventory.json")
    print(keywords)
    # keywords = ["çŒªè‚‰", "ç‰›è‚‰", "ç¾Šè‚‰"]

# å­˜æ”¾æ‰€æœ‰ç»“æœ
    all_results = []

    for kw in keywords:
        result = pipeline_fn(f"ä¸»æ–™ï¼š{kw}")
        parsed = extract_answer(result)
        all_results.append(parsed)

# è¾“å‡ºæ‰€æœ‰ JSON ç»“æœ
    print('____________', all_results)


#    predictions = predict_answers(pipeline_fn, questions)